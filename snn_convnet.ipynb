{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Recognition with SNNs\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import snntorch as snn\n",
    "from snntorch import functional as SF\n",
    "\n",
    "import wandb\n",
    "import lightning as L\n",
    "import torchsummary\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from utils import create_sample, make_event_based, animate, spiking_overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"dataset\": {\n",
    "        \"n_samples\": 2000,\n",
    "        \"shapes\": ('circle', 'square'),\n",
    "        \"frame_size\": 64,\n",
    "        \"n_frames\": 16,\n",
    "    },\n",
    "    \"epochs\": 1,\n",
    "    \"population\": 1,\n",
    "    \"conv_layers\": {\n",
    "        \"input_channels\": (1, 16),\n",
    "        \"output_channels\": (16, 32),\n",
    "        \"kernel_sizes\": (3, 3),\n",
    "        \"paddings\": (\"same\", \"same\")\n",
    "    },\n",
    "    \"max_pool_layers\": {\n",
    "        \"kernel_sizes\": (2, 2),\n",
    "        \"strides\": (2, 2)\n",
    "    },\n",
    "    \"leaky_layers\": {\n",
    "        \"betas\": (0.95, 0.95, 0.95),\n",
    "        \"learn_betas\": (False, False, False)\n",
    "    },\n",
    "    \"fc_layer\": {\n",
    "        \"input_channels\": 32*16*16,\n",
    "        \"output_channels\": 5\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"lr\": 1e-2,\n",
    "        \"betas\": (0.9, 0.999)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data\n",
    "Just a gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = \"square\"\n",
    "motion = \"rotation\"\n",
    "frames, label = create_sample(shape, motion, config[\"dataset\"][\"frame_size\"], config[\"dataset\"][\"n_frames\"])\n",
    "animate(frames, filename=f\"{shape}_{motion}_frames.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert it to events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = make_event_based(frames)\n",
    "animate(events, filename=f\"{shape}_{motion}_events.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventBasedDataset(Dataset):\n",
    "    def __init__(self, samples, config):\n",
    "        self.samples = samples\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        shape = np.random.choice(self.config[\"dataset\"][\"shapes\"])\n",
    "        motion = np.random.choice([\"up\", \"down\", \"left\", \"right\", \"rotation\"])\n",
    "        frames, label = create_sample(shape, motion, self.config[\"dataset\"][\"frame_size\"], self.config[\"dataset\"][\"n_frames\"])\n",
    "        events = make_event_based(frames)\n",
    "        return torch.from_numpy(events).type(torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = EventBasedDataset(config[\"dataset\"][\"n_samples\"], config)\n",
    "val_dataset = EventBasedDataset(config[\"dataset\"][\"n_samples\"]//100, config)\n",
    "test_dataset = EventBasedDataset(config[\"dataset\"][\"n_samples\"]//10, config)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningConvNet(L.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters(config)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.hparams.conv_layers[\"input_channels\"][0], self.hparams.conv_layers[\"output_channels\"][0], kernel_size=self.hparams.conv_layers[\"kernel_sizes\"][0], padding=self.hparams.conv_layers[\"paddings\"][0])\n",
    "        self.lif1 = snn.Leaky(beta=self.hparams.leaky_layers[\"betas\"][0], learn_beta=self.hparams.leaky_layers[\"learn_betas\"][0])\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=self.hparams.max_pool_layers[\"kernel_sizes\"][0], stride=self.hparams.max_pool_layers[\"strides\"][0])\n",
    "\n",
    "        self.conv2 = nn.Conv2d(self.hparams.conv_layers[\"input_channels\"][1], self.hparams.conv_layers[\"output_channels\"][1], kernel_size=self.hparams.conv_layers[\"kernel_sizes\"][1], padding=self.hparams.conv_layers[\"paddings\"][1])\n",
    "        self.lif2 = snn.Leaky(beta=self.hparams.leaky_layers[\"betas\"][1], learn_beta=self.hparams.leaky_layers[\"learn_betas\"][1])\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=self.hparams.max_pool_layers[\"kernel_sizes\"][1], stride=self.hparams.max_pool_layers[\"strides\"][1])\n",
    "\n",
    "        self.fc1 = nn.Linear(self.hparams.fc_layer[\"input_channels\"], self.hparams.fc_layer[\"output_channels\"]*self.hparams.population)\n",
    "        self.lif3 = snn.Leaky(beta=self.hparams.leaky_layers[\"betas\"][2], learn_beta=self.hparams.leaky_layers[\"learn_betas\"][2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        spk1_rec = []\n",
    "        mem1_rec = []\n",
    "\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        spk3_rec = []\n",
    "        mem3_rec = []\n",
    "\n",
    "        # (B, T, H, W) -> (B, C, T, H, W) where C = 1\n",
    "        if len(x.shape) == 4:\n",
    "            x = x.unsqueeze(1)\n",
    "            steps = x.shape[2]\n",
    "        # (T, H, W) -> (B, C, T, H, W) where B = C = 1\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(0).unsqueeze(0)\n",
    "            steps = x.shape[2]\n",
    "\n",
    "        for step in range(steps):\n",
    "            x_step = x[:, :, step]\n",
    "\n",
    "            cur1 = self.conv1(x_step)\n",
    "            spk1, mem1 = self.lif1(self.pool1(cur1), mem1)\n",
    "            spk1_rec.append(spk1)\n",
    "            mem1_rec.append(mem1)\n",
    "\n",
    "            cur2 = self.conv2(spk1)\n",
    "            spk2, mem2 = self.lif2(self.pool2(cur2), mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "            cur3 = self.fc1(spk2.flatten(1))\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            spk3_rec.append(spk3)\n",
    "            mem3_rec.append(mem3)\n",
    "\n",
    "        return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0), torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0), torch.stack(spk1_rec, dim=0), torch.stack(mem1_rec, dim=0)\n",
    "\n",
    "    def common_step(self, batch, batch_idx, split):\n",
    "        data, targets = batch\n",
    "        spk_rec, _, _, _, _, _ = self(data)\n",
    "        if self.hparams.population == 1:\n",
    "            loss = nn.CrossEntropyLoss()\n",
    "            loss_val = loss(spk_rec.sum(0), targets)\n",
    "        else:\n",
    "            loss = SF.ce_count_loss(population_code=True, num_classes=5)\n",
    "            loss_val = loss(spk_rec, targets)\n",
    "        acc = (spk_rec.sum(0).argmax(-1) == targets).float().mean()\n",
    "\n",
    "        # logging\n",
    "        self.log(f\"{split}/loss\", loss_val)\n",
    "        self.log(f\"{split}/acc\", acc)\n",
    "\n",
    "        return loss_val\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss_val = self.common_step(batch, batch_idx, \"train\")\n",
    "        return loss_val\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss_val = self.common_step(batch, batch_idx, \"val\")\n",
    "        return loss_val\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss_val = self.common_step(batch, batch_idx, \"test\")\n",
    "        return loss_val\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.optimizer[\"lr\"], betas=self.hparams.optimizer[\"betas\"])\n",
    "        return optimizer\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"SNN-motion-recognition\")\n",
    "trainer = L.Trainer(max_epochs=config[\"epochs\"], logger=wandb_logger, callbacks=[EarlyStopping(monitor=\"val/loss\", mode=\"min\", patience=5)])\n",
    "model = LightningConvNet(config)\n",
    "# torchsummary.summary(model, (config[\"dataset\"][\"n_frames\"], config[\"dataset\"][\"frame_size\"], config[\"dataset\"][\"frame_size\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "save = False\n",
    "\n",
    "if train:\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "    if save:\n",
    "        torch.save(model.state_dict(), 'neww.pth')\n",
    "else:\n",
    "    # load model from .pth file\n",
    "    model.load_state_dict(torch.load('models/tst.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, test_dataloader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(convnet, dataloader, population):\n",
    "  with torch.no_grad():\n",
    "      convnet.eval()\n",
    "      running_accuracy = 0\n",
    "      for data, targets in iter(dataloader):\n",
    "          data = data.to(device)\n",
    "          targets = targets.to(device)\n",
    "\n",
    "          spk_rec, _, _, _, _, _ = convnet(data)\n",
    "          if population == 1:\n",
    "              running_accuracy += SF.accuracy_rate(spk_rec, targets)\n",
    "          else:\n",
    "              running_accuracy += SF.accuracy_rate(spk_rec, targets, population_code=True, num_classes=5)\n",
    "      \n",
    "      accuracy = running_accuracy / len(dataloader)\n",
    "      \n",
    "      return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = get_accuracy(model, test_dataloader, config[\"population\"])\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = \"square\"\n",
    "motion = \"left\"\n",
    "frames, label = create_sample(shape, motion, config[\"dataset\"][\"frame_size\"], config[\"dataset\"][\"n_frames\"])\n",
    "events = make_event_based(frames)\n",
    "spk3, mem3, spk2, mem2, spk1, mem1 = model(torch.from_numpy(events).type(torch.float32))\n",
    "# print(spk3.shape, mem3.shape, spk2.shape, mem2.shape, spk1.shape, mem1.shape)\n",
    "spks = [spk1.detach().numpy().squeeze(1), spk2.detach().numpy().squeeze(1), spk3.detach().numpy().squeeze(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "filename = 'spiking_overview'\n",
    "spiking_overview(spks, events, config[\"dataset\"][\"frame_size\"], filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
