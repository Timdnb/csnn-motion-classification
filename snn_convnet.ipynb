{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Recognition with SNNs\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import snntorch as snn\n",
    "from snntorch import functional as SF\n",
    "\n",
    "from utils import create_sample, make_event_based, animate, spiking_overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "frame_size = 64\n",
    "n_frames = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample\n",
    "shape = \"square\"\n",
    "motion = \"rotation\"\n",
    "frames, label = create_sample(shape, motion, frame_size, n_frames)\n",
    "animate(frames, filename=f\"{shape}_{motion}_frames.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = make_event_based(frames)\n",
    "animate(events, filename=f\"{shape}_{motion}_events.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventBasedDataset(Dataset):\n",
    "    def __init__(self, samples, frame_size, n_frames):\n",
    "        self.samples = samples\n",
    "        self.frame_size = frame_size\n",
    "        self.n_frames = n_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        shape = np.random.choice([\"circle\", \"square\"])\n",
    "        motion = np.random.choice([\"up\", \"down\", \"left\", \"right\", \"rotation\"])\n",
    "        frames, label = create_sample(shape, motion, self.frame_size, self.n_frames)\n",
    "        events = make_event_based(frames)\n",
    "        return torch.from_numpy(events).type(torch.float32), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, population=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.population = population\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=\"same\")\n",
    "        self.lif1 = snn.Leaky(beta=0.95, learn_beta=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=\"same\")\n",
    "        self.lif2 = snn.Leaky(beta=0.95, learn_beta=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(32*16*16, 5*self.population)\n",
    "        self.lif3 = snn.Leaky(beta=0.95, learn_beta=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        spk1_rec = []\n",
    "        mem1_rec = []\n",
    "\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        spk3_rec = []\n",
    "        mem3_rec = []\n",
    "\n",
    "        # (B, T, H, W) -> (B, C, T, H, W) where C = 1\n",
    "        if len(x.shape) == 4:\n",
    "            x = x.unsqueeze(1)\n",
    "            steps = x.shape[2]\n",
    "        # (T, H, W) -> (B, C, T, H, W) where B = C = 1\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(0).unsqueeze(0)\n",
    "            steps = x.shape[2]\n",
    "        \n",
    "        for step in range(steps):\n",
    "            x_step = x[:, :, step]\n",
    "            # print(x_step.shape)\n",
    "            cur1 = self.conv1(x_step)\n",
    "            spk1, mem1 = self.lif1(self.pool1(cur1), mem1)\n",
    "            spk1_rec.append(spk1)\n",
    "            mem1_rec.append(mem1)\n",
    "\n",
    "            cur2 = self.conv2(spk1)\n",
    "            spk2, mem2 = self.lif2(self.pool2(cur2), mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "            cur3 = self.fc1(spk2.flatten(1))\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            spk3_rec.append(spk3)\n",
    "            mem3_rec.append(mem3)\n",
    "\n",
    "        return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0), torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0), torch.stack(spk1_rec, dim=0), torch.stack(mem1_rec, dim=0)\n",
    "    \n",
    "def get_accuracy(convnet, dataloader, population):\n",
    "  with torch.no_grad():\n",
    "      convnet.eval()\n",
    "      running_accuracy = 0\n",
    "      for data, targets in iter(dataloader):\n",
    "          data = data.to(device)\n",
    "          targets = targets.to(device)\n",
    "\n",
    "          spk_rec, _, _, _, _, _ = convnet(data)\n",
    "          if population == 1:\n",
    "              running_accuracy += SF.accuracy_rate(spk_rec, targets)\n",
    "          else:\n",
    "              running_accuracy += SF.accuracy_rate(spk_rec, targets, population_code=True, num_classes=5)\n",
    "      \n",
    "      accuracy = running_accuracy / len(dataloader)\n",
    "      \n",
    "      return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False\n",
    "samples = 10000\n",
    "population = 1\n",
    "\n",
    "convnet = ConvNet(population).to(device)\n",
    "\n",
    "# Create a dataloaders\n",
    "train_dataset = EventBasedDataset(samples, frame_size, n_frames)\n",
    "val_dataset = EventBasedDataset(samples//100, frame_size, n_frames)\n",
    "test_dataset = EventBasedDataset(samples//10, frame_size, n_frames)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "if train:\n",
    "    if population == 1:\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        loss = SF.ce_count_loss(population_code=True, num_classes=5)\n",
    "\n",
    "    optimizer = torch.optim.Adam(convnet.parameters(), lr=1e-2, betas=(0.9, 0.999))\n",
    "\n",
    "    num_epochs = 1\n",
    "    loss_hist = []\n",
    "    test_loss_hist = []\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_batch = iter(train_dataloader)\n",
    "\n",
    "        for data, targets in train_batch:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            convnet.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            spk_rec, _, _, _, _, _ = convnet(data)\n",
    "            if population == 1:\n",
    "                loss_val = loss(spk_rec.sum(0), targets)\n",
    "            else:\n",
    "                loss_val = loss(spk_rec, targets)\n",
    "            \n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_hist.append(loss_val.item())\n",
    "\n",
    "            if counter % 10 == 0:\n",
    "                print(f\"Epoch: {epoch}, Counter: {counter}, Loss: {loss_val.item()}, Val Acc: {get_accuracy(convnet, val_dataloader, population)}\")\n",
    "\n",
    "            counter += 1\n",
    "else:\n",
    "    # load model from .pth file\n",
    "    convnet.load_state_dict(torch.load('models/model-25_6k.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = get_accuracy(convnet, test_dataloader, population)\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = \"square\"\n",
    "motion = \"right\"\n",
    "frames, label = create_sample(shape, motion, frame_size, n_frames)\n",
    "events = make_event_based(frames)\n",
    "spk3, mem3, spk2, mem2, spk1, mem1 = convnet(torch.from_numpy(events).type(torch.float32))\n",
    "# print(spk3.shape, mem3.shape, spk2.shape, mem2.shape, spk1.shape, mem1.shape)\n",
    "spks = [spk1.detach().numpy().squeeze(1), spk2.detach().numpy().squeeze(1), spk3.detach().numpy().squeeze(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "filename = 'spiking_overview'\n",
    "spiking_overview(spks, events, frame_size, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
