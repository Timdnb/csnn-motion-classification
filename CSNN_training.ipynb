{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Recognition with CSNNs\n",
    "Author: Tim den Blanken (t.n.a.denblanken@student.tudelft.nl)\n",
    "\n",
    "This notebook is used to train and investigate convolutional spiking neural networks (CSNNs) and their ability to classify (planar) motions or rotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "Import the libraries needed, together with some functions from the `utils.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # When running in Google Colab or Kaggle, uncomment the following lines\n",
    "# !pip install torch --quiet\n",
    "# !pip install lightning --quiet\n",
    "# !pip install snntorch --quiet\n",
    "# !pip install wandb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import snntorch as snn\n",
    "from snntorch import functional as SF\n",
    "\n",
    "import wandb\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from utils import create_sample, make_event_based, animate, spiking_overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Some preparation\n",
    "Set seeds for reproducibility and assign the correct device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "project_name = \"CSNN-motion-classification\" # for wandb\n",
    "\n",
    "if not os.path.exists(\"animations\"):\n",
    "    os.makedirs(\"animations\")\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parameter dashboard\n",
    "These are all parameters that govern the model and dataset. Change the parameters here, run the notebook and see the effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"dataset\": {\n",
    "        \"n_samples\": 32000,\n",
    "        \"shapes_train\": ('square', 'circle'),\n",
    "        \"shapes_test\": ('square', 'circle'),\n",
    "        \"frame_size\": 64,\n",
    "        \"n_frames\": 16,\n",
    "    },\n",
    "    \"epochs\": 1,\n",
    "    \"population\": 1,\n",
    "    \"conv_layers\": {\n",
    "        \"input_channels\": (1, 16),\n",
    "        \"output_channels\": (16, 32),\n",
    "        \"kernel_sizes\": (3, 3),\n",
    "        \"paddings\": (\"same\", \"same\")\n",
    "    },\n",
    "    \"max_pool_layers\": {\n",
    "        \"kernel_sizes\": (2, 2),\n",
    "        \"strides\": (2, 2)\n",
    "    },\n",
    "    \"leaky_layers\": {\n",
    "        \"betas\": (0.95, 0.95, 0.95),\n",
    "        \"learn_betas\": (True, True, True)\n",
    "    },\n",
    "    \"fc_layer\": {\n",
    "        \"input_channels\": None,\n",
    "        \"output_channels\": 5\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"lr\": 1e-2,\n",
    "        \"betas\": (0.9, 0.999)\n",
    "    }\n",
    "}\n",
    "\n",
    "logging = False      # if you have not connected wandb, set this to False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of input channels for the fully connected layer depends on the other parameters, we calculate it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_maxp_1 = ((config[\"dataset\"][\"frame_size\"] - config[\"max_pool_layers\"][\"kernel_sizes\"][0]) // config[\"max_pool_layers\"][\"strides\"][0]) + 1\n",
    "after_maxp_2 = ((after_maxp_1 - config[\"max_pool_layers\"][\"kernel_sizes\"][1]) // config[\"max_pool_layers\"][\"strides\"][1]) + 1\n",
    "config[\"fc_layer\"][\"input_channels\"] = after_maxp_2 * after_maxp_2 * config[\"conv_layers\"][\"output_channels\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize data\n",
    "Let's see what the data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = \"square\"    # this can also be \"circle\" or \"noise\"\n",
    "motions = [\"up\", \"down\", \"left\", \"right\", \"rotation\"]\n",
    "frames_list = []\n",
    "labels_list = []\n",
    "for motion in motions:\n",
    "    frames, labels = create_sample(shape, motion, config[\"dataset\"][\"frame_size\"], config[\"dataset\"][\"n_frames\"])\n",
    "    frames_list.append(frames)\n",
    "    labels_list.append(labels)\n",
    "\n",
    "animate(frames_list, \"all_motions.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 4.1: All motions in normal format</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert it to events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_list = []\n",
    "for frames in frames_list:\n",
    "    events = make_event_based(frames)\n",
    "    events_list.append(events)\n",
    "    \n",
    "animate(events_list, \"all_motions_events.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 4.2: All motions in event-based format</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create dataloaders\n",
    "Now we create dataloaders that generate samples just like the ones above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventBasedDataset(Dataset):\n",
    "    def __init__(self, samples, config, split):\n",
    "        self.samples = samples\n",
    "        self.config = config\n",
    "        self.split = split\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.split == \"train\" or self.split == \"val\":\n",
    "          shape = np.random.choice(self.config[\"dataset\"][\"shapes_train\"])\n",
    "        if self.split == \"test\":\n",
    "          shape = np.random.choice(self.config[\"dataset\"][\"shapes_test\"])\n",
    "        motion = np.random.choice([\"up\", \"down\", \"left\", \"right\", \"rotation\"])\n",
    "        frames, label = create_sample(shape, motion, self.config[\"dataset\"][\"frame_size\"], self.config[\"dataset\"][\"n_frames\"])\n",
    "        events = make_event_based(frames)\n",
    "        return torch.from_numpy(events).type(torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = EventBasedDataset(config[\"dataset\"][\"n_samples\"], config, \"train\")\n",
    "val_dataset = EventBasedDataset(config[\"dataset\"][\"n_samples\"]//7, config, \"val\")\n",
    "test_dataset = EventBasedDataset(config[\"dataset\"][\"n_samples\"]//10, config, \"test\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The model\n",
    "Next up we create the model based on the parameters we defined before. In the forward step we save data about the spikes and membrane states, such that we can visualize those to see what the model actually is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningConvNet(L.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters(config)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.hparams.conv_layers[\"input_channels\"][0], self.hparams.conv_layers[\"output_channels\"][0], kernel_size=self.hparams.conv_layers[\"kernel_sizes\"][0], padding=self.hparams.conv_layers[\"paddings\"][0])\n",
    "        self.lif1 = snn.Leaky(beta=self.hparams.leaky_layers[\"betas\"][0], learn_beta=self.hparams.leaky_layers[\"learn_betas\"][0])\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=self.hparams.max_pool_layers[\"kernel_sizes\"][0], stride=self.hparams.max_pool_layers[\"strides\"][0])\n",
    "\n",
    "        self.conv2 = nn.Conv2d(self.hparams.conv_layers[\"input_channels\"][1], self.hparams.conv_layers[\"output_channels\"][1], kernel_size=self.hparams.conv_layers[\"kernel_sizes\"][1], padding=self.hparams.conv_layers[\"paddings\"][1])\n",
    "        self.lif2 = snn.Leaky(beta=self.hparams.leaky_layers[\"betas\"][1], learn_beta=self.hparams.leaky_layers[\"learn_betas\"][1])\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=self.hparams.max_pool_layers[\"kernel_sizes\"][1], stride=self.hparams.max_pool_layers[\"strides\"][1])\n",
    "\n",
    "        self.fc1 = nn.Linear(self.hparams.fc_layer[\"input_channels\"], self.hparams.fc_layer[\"output_channels\"]*self.hparams.population)\n",
    "        self.lif3 = snn.Leaky(beta=self.hparams.leaky_layers[\"betas\"][2], learn_beta=self.hparams.leaky_layers[\"learn_betas\"][2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        spk1_rec = []\n",
    "        mem1_rec = []\n",
    "\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        spk3_rec = []\n",
    "        mem3_rec = []\n",
    "\n",
    "        # (B, T, H, W) -> (B, C, T, H, W) where C = 1\n",
    "        if len(x.shape) == 4:\n",
    "            x = x.unsqueeze(1)\n",
    "            steps = x.shape[2]\n",
    "        # (T, H, W) -> (B, C, T, H, W) where B = C = 1\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(0).unsqueeze(0)\n",
    "            steps = x.shape[2]\n",
    "\n",
    "        for step in range(steps):\n",
    "            x_step = x[:, :, step]\n",
    "\n",
    "            cur1 = self.conv1(x_step)\n",
    "            spk1, mem1 = self.lif1(self.pool1(cur1), mem1)\n",
    "            spk1_rec.append(spk1)\n",
    "            mem1_rec.append(mem1)\n",
    "\n",
    "            cur2 = self.conv2(spk1)\n",
    "            spk2, mem2 = self.lif2(self.pool2(cur2), mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "            cur3 = self.fc1(spk2.flatten(1))\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            spk3_rec.append(spk3)\n",
    "            mem3_rec.append(mem3)\n",
    "\n",
    "        return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0), torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0), torch.stack(spk1_rec, dim=0), torch.stack(mem1_rec, dim=0)\n",
    "\n",
    "    def common_step(self, batch, batch_idx, split):\n",
    "        data, targets = batch\n",
    "        spk_rec, _, _, _, _, _ = self(data)\n",
    "        if self.hparams.population == 1:\n",
    "            loss = nn.CrossEntropyLoss()\n",
    "            loss_val = loss(spk_rec.sum(0), targets)\n",
    "            acc = (spk_rec.sum(0).argmax(-1) == targets).float().mean()\n",
    "        else:\n",
    "            loss = SF.ce_count_loss(population_code=True, num_classes=5)\n",
    "            loss_val = loss(spk_rec, targets)\n",
    "            spk_rec_reshaped = spk_rec.view(-1, spk_rec.shape[1], 5, self.hparams.population)\n",
    "            spr_rec_summed = spk_rec_reshaped.sum(-1)\n",
    "            acc = (spr_rec_summed.sum(0).argmax(-1) == targets).float().mean()\n",
    "\n",
    "        # logging\n",
    "        self.log(f\"{split}/loss\", loss_val)\n",
    "        self.log(f\"{split}/acc\", acc)\n",
    "\n",
    "        return loss_val\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss_val = self.common_step(batch, batch_idx, \"train\")\n",
    "        return loss_val\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss_val = self.common_step(batch, batch_idx, \"val\")\n",
    "        return loss_val\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss_val = self.common_step(batch, batch_idx, \"test\")\n",
    "        return loss_val\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.optimizer[\"lr\"], betas=self.hparams.optimizer[\"betas\"])\n",
    "        return optimizer\n",
    "\n",
    "if logging:\n",
    "    wandb.login()\n",
    "    wandb_logger = WandbLogger(project=project_name)\n",
    "    trainer = L.Trainer(max_epochs=config[\"epochs\"], logger=wandb_logger, callbacks=[EarlyStopping(monitor=\"val/loss\", mode=\"min\", patience=5)])\n",
    "else:\n",
    "    trainer = L.Trainer(max_epochs=config[\"epochs\"], callbacks=[EarlyStopping(monitor=\"val/loss\", mode=\"min\", patience=5)])\n",
    "\n",
    "# Create model\n",
    "model = LightningConvNet(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train\n",
    "Now it's time to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True   # set to True if you want to train the model, False if you want to load a model\n",
    "save = False   # set to True if you want to save the model\n",
    "\n",
    "if train:\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "    if save:\n",
    "        torch.save(model.state_dict(), 'models/model.pth')\n",
    "else:\n",
    "    # load model from .pth file\n",
    "    model.load_state_dict(torch.load('models/model.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test\n",
    "And finally let's test the model and see what it is capable of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    trainer.test(model, test_dataloader)\n",
    "    if logging:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the data about spikes and membranes that is saved during the forward pass can be used to visualize the spiking activity. Simply create a sample below and generate the plot. It will use the model that you trained or loaded before. Note: the order of the final five output spikes is from top to bottom: \"up\", \"down\", \"left\", \"right\" and \"rotation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = \"square\"\n",
    "motion = \"rotation\"\n",
    "\n",
    "frames, label = create_sample(shape, motion, config[\"dataset\"][\"frame_size\"], config[\"dataset\"][\"n_frames\"])\n",
    "events = make_event_based(frames)\n",
    "spk3, mem3, spk2, mem2, spk1, mem1 = model(torch.from_numpy(events).type(torch.float32))\n",
    "spks = [spk1.detach().numpy().squeeze(1), spk2.detach().numpy().squeeze(1), spk3.detach().numpy().squeeze(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'spiking_overview'\n",
    "spiking_overview(spks, events, config[\"dataset\"][\"frame_size\"], filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
